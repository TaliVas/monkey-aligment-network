{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1da85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "with open('../features/Features_thina', 'rb') as f:\n",
    "    df_features,start_move,stop_turn = pickle.load(f)\n",
    "\n",
    "print(f\"Numver of features: {len(df_features['id'].unique())}\")\n",
    "df_features = df_features[(df_features['normalized_start_movement'].notna()) & (df_features['normalized_start_movement'] > 0.0005)]\n",
    "print(f\"Number of features after filtering: {len(df_features['id'].unique())}\")\n",
    "#df_features = df_features[df_features['normalized_stop_turn'].notnull()]\n",
    "print(f\"Number of start movements before filtering: {len(start_move)}\")\n",
    "start_move = {k: v for k, v in start_move.items() if v > 0.0005 and not pd.isna(v)}\n",
    "print(f\"Number of start movements after filtering: {len(start_move)}\")\n",
    "dict_list = [('start_move', start_move)]\n",
    "\n",
    "# Initialize an empty DataFrame for the result\n",
    "df_annotated = pd.DataFrame()\n",
    "# Iterate over the list of tuples to create and merge each DataFrame\n",
    "for feature, data in dict_list:\n",
    "    # Convert dictionary to DataFrame and reset index\n",
    "    df_temp = pd.DataFrame.from_dict(data, orient='index', columns=[feature]).reset_index()\n",
    "    df_temp.columns = ['id', feature]  # Rename columns\n",
    "    # Merge the temporary DataFrame with the main annotated DataFrame\n",
    "    if df_annotated.empty:\n",
    "        df_annotated = df_temp\n",
    "    else:\n",
    "        df_annotated = df_annotated.merge(df_temp, on='id', how='outer')\n",
    "features = ['rotation_x', 'rotation_y','rotation_z','normalized_time','velocity','angle','distance_to_end']\n",
    "max_time = df_features.groupby('id')['adjusted_time'].max()\n",
    "min_time = df_features.groupby('id')['adjusted_time'].min()\n",
    "\n",
    "X = df_features.drop(['type_trajectory'], axis=1)\n",
    "grouped = X.groupby('id')\n",
    "trajectories = [group[features].values for _, group in grouped]\n",
    "ids = list(grouped.groups.keys())\n",
    "trajectory_type = df_features.groupby('id')['type_trajectory'].first().values\n",
    "unique_trajectory_types = np.unique(trajectory_type)\n",
    "trajectory_type_to_index = {t: i for i, t in enumerate(unique_trajectory_types)}\n",
    "trajectory_type_indices = np.array([trajectory_type_to_index[t] for t in trajectory_type])\n",
    "num_trajectory_types = len(unique_trajectory_types)\n",
    "\n",
    "# Scale the data - Column-wise scaling across all trajectories\n",
    "def scale_trajectories_columnwise(trajectories):\n",
    "    if not trajectories:\n",
    "        return trajectories\n",
    "    all_trajectories = np.array(trajectories, dtype=object)\n",
    "    num_features = trajectories[0].shape[1] if len(trajectories[0]) > 0 else 0\n",
    "    if num_features == 0:\n",
    "        return trajectories\n",
    "    all_values_per_feature = [[] for _ in range(num_features)]\n",
    "    for traj in trajectories:\n",
    "        if len(traj) > 0:\n",
    "            for feature_idx in range(num_features):\n",
    "                all_values_per_feature[feature_idx].extend(traj[:, feature_idx])\n",
    "    feature_means = []\n",
    "    feature_stds = []\n",
    "    for feature_idx in range(num_features):\n",
    "        values = np.array(all_values_per_feature[feature_idx])\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        feature_means.append(mean_val)\n",
    "        feature_stds.append(std_val if std_val > 0 else 1.0)\n",
    "    feature_means = np.array(feature_means)\n",
    "    feature_stds = np.array(feature_stds)\n",
    "    scaled_trajectories = []\n",
    "    for traj in trajectories:\n",
    "        if len(traj) == 0:\n",
    "            scaled_trajectories.append(traj)\n",
    "        else:\n",
    "            scaled_traj = (traj - feature_means) / feature_stds\n",
    "            scaled_trajectories.append(scaled_traj)\n",
    "    return scaled_trajectories, feature_means, feature_stds\n",
    "\n",
    "def scale_test_trajectories(test_trajectories, means, stds):\n",
    "    scaled_trajectories = []\n",
    "    for traj in test_trajectories:\n",
    "        if len(traj) == 0:\n",
    "            scaled_trajectories.append(traj)\n",
    "        else:\n",
    "            scaled_traj = (traj - means) / stds\n",
    "            scaled_trajectories.append(scaled_traj)\n",
    "    return scaled_trajectories\n",
    "\n",
    "def pad_sequences(sequences, maxlen, dtype='float32', padding='post', value=0):\n",
    "    padded_sequences = np.full((len(sequences), maxlen, len(sequences[0][0])), value, dtype=dtype)\n",
    "    for i, seq in enumerate(sequences):\n",
    "        if len(seq) > maxlen:\n",
    "            padded_sequences[i] = seq[:maxlen]\n",
    "        else:\n",
    "            padded_sequences[i, :len(seq)] = seq\n",
    "    return padded_sequences\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, sequences, types, labels, device):\n",
    "        self.sequences = torch.tensor(sequences, dtype=torch.float32).to(device)\n",
    "        self.types = torch.tensor(types, dtype=torch.int64).to(device)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.types[idx], self.labels[idx]\n",
    "\n",
    "class TrajectoryModel(nn.Module):\n",
    "    def __init__(self, num_trajectory_types, number_of_features, sequence_length):\n",
    "        super(TrajectoryModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(number_of_features, 50, bidirectional=True, batch_first=True)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.embedding = nn.Embedding(num_trajectory_types, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(100 + 2, 100)\n",
    "        self.fc_mean = nn.Linear(100, 1)\n",
    "        self.fc_logvar = nn.Linear(100, 1)\n",
    "    def forward(self, x, type_input):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        lstm_out = lstm_out.permute(0, 2, 1)\n",
    "        avg_pool_out = self.avg_pool(lstm_out).squeeze(-1)\n",
    "        embedded_type = self.embedding(type_input)\n",
    "        embedded_type = self.flatten(embedded_type)\n",
    "        combined = torch.cat((avg_pool_out, embedded_type), dim=1)\n",
    "        x = torch.relu(self.fc1(combined))\n",
    "        mu = self.fc_mean(x)\n",
    "        logvar = self.fc_logvar(x)\n",
    "        return mu, logvar\n",
    "\n",
    "def nll_criterion_gaussian(mu, logvar, target, reduction='mean'):\n",
    "    loss = torch.exp(-logvar) * torch.pow(target - mu, 2) + logvar\n",
    "    return loss.mean() if reduction == 'mean' else loss.sum()\n",
    "\n",
    "def calculate_mae(outputs, labels, ids, max_time, min_time):\n",
    "    total_mae = 0.0\n",
    "    num_samples = 0\n",
    "    for pred, lbl in zip(outputs, labels):\n",
    "        _id = ids[num_samples]\n",
    "        true_value_ms = lbl.item() * (max_time[_id] - min_time[_id]) + min_time[_id]\n",
    "        pred_value_ms = pred.item() * (max_time[_id] - min_time[_id]) + min_time[_id]\n",
    "        total_mae += abs(pred_value_ms - true_value_ms)\n",
    "        num_samples += 1\n",
    "    return total_mae, num_samples\n",
    "\n",
    "# Repeat train/test split, training, and evaluation 10 times\n",
    "num_runs = 1\n",
    "mae_results = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"\\nRun {run+1}/{num_runs}\")\n",
    "    # Split the data\n",
    "    X_train, X_test, id_train, id_test, type_train_lst, type_test_lst = train_test_split(\n",
    "        trajectories, ids, trajectory_type_indices, test_size=0.2, random_state=42 + run)\n",
    "    type_train = np.array(type_train_lst)\n",
    "    type_test = np.array(type_test_lst)\n",
    "\n",
    "    # Scale training and test data\n",
    "    X_train_scaled, train_means, train_stds = scale_trajectories_columnwise(X_train)\n",
    "    X_test_scaled = scale_test_trajectories(X_test, train_means, train_stds)\n",
    "\n",
    "    max_sequence_length = max([len(sequence) for sequence in X_train])\n",
    "    X_train_padded = pad_sequences(X_train_scaled, max_sequence_length, padding='post')\n",
    "    X_test_padded = pad_sequences(X_test_scaled, max_sequence_length, padding='post')\n",
    "    sequence_length = X_train_padded.shape[1]\n",
    "    number_of_features = X_train_padded.shape[2]\n",
    "\n",
    "    # Use CPU only for training and evaluation to avoid MPS memory issues\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    for name, label in dict_list:\n",
    "        y_train = np.array([label[_id] for _id in id_train])\n",
    "        y_test = np.array([label[_id] for _id in id_test])\n",
    "\n",
    "        train_dataset = TrajectoryDataset(X_train_padded, type_train, y_train, device)\n",
    "        test_dataset = TrajectoryDataset(X_test_padded, type_test, y_test, device)\n",
    "\n",
    "        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "        model = TrajectoryModel(num_trajectory_types, number_of_features, sequence_length).to(device)\n",
    "        criterion = nll_criterion_gaussian\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "        epochs = 100\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for sequences, types, labels in train_loader:\n",
    "                sequences, types, labels = sequences.to(device), types.to(device), labels.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                mu, logvar = model(sequences, types)\n",
    "                loss = nll_criterion_gaussian(mu.squeeze(), logvar.squeeze(), labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "            for sequences, types, labels in test_loader:\n",
    "                sequences, types, labels = sequences.to(device), types.to(device), labels.to(device)\n",
    "                mu, logvar = model(sequences, types)\n",
    "                all_preds.extend(mu.squeeze().cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calculate MAE for this run\n",
    "        total_mae = 0.0\n",
    "        for pred, lbl, _id in zip(all_preds, all_labels, id_test):\n",
    "            true_value_ms = lbl * (max_time[_id] - min_time[_id]) + min_time[_id]\n",
    "            pred_value_ms = pred * (max_time[_id] - min_time[_id]) + min_time[_id]\n",
    "            total_mae += abs(pred_value_ms - true_value_ms)\n",
    "        mean_mae = total_mae / len(id_test)\n",
    "        print(f\"Run {run+1} MAE: {mean_mae:.4f}\")\n",
    "        mae_results.append(mean_mae)\n",
    "\n",
    "        # ===============================================\n",
    "        # EVALUATION ON NANA'S TRAILS\n",
    "        # ===============================================\n",
    "\n",
    "        print(\"Loading Nana's data for evaluation...\")\n",
    "\n",
    "        # Load Nana's features\n",
    "        with open('all_features_3d_aligned_NANA', 'rb') as f:\n",
    "            df_features_nana, start_move_nana, stop_turn_nana = pickle.load(f)\n",
    "\n",
    "\n",
    "        print(f\"Numver of features: {len(df_features_nana['id'].unique())}\")\n",
    "        df_features_nana = df_features_nana[(df_features_nana['normalized_start_movement'].notna()) & (df_features_nana['normalized_start_movement'] > 0.0005)]\n",
    "        print(f\"Number of features after filtering: {len(df_features_nana['id'].unique())}\")\n",
    "        #df_features = df_features[df_features['normalized_stop_turn'].notnull()]\n",
    "        print(f\"Number of start movements before filtering: {len(start_move_nana)}\")\n",
    "        start_move_nana = {k: v for k, v in start_move_nana.items() if v > 0.0005 and not pd.isna(v)}\n",
    "        print(f\"Number of start movements after filtering: {len(start_move_nana)}\")\n",
    "        dict_list_nana = [('start_move', start_move_nana)]\n",
    "\n",
    "\n",
    "        df_annotated_nana = pd.DataFrame()\n",
    "        for feature, data in dict_list_nana:\n",
    "            df_temp = pd.DataFrame.from_dict(data, orient='index', columns=[feature]).reset_index()\n",
    "            df_temp.columns = ['id', feature]\n",
    "            if df_annotated_nana.empty:\n",
    "                df_annotated_nana = df_temp\n",
    "            else:\n",
    "                df_annotated_nana = df_annotated_nana.merge(df_temp, on='id', how='outer')\n",
    "\n",
    "        # Calculate time ranges for Nana data (needed for denormalization)\n",
    "        max_time_nana = df_features_nana.groupby('id')['adjusted_time'].max()\n",
    "        min_time_nana = df_features_nana.groupby('id')['adjusted_time'].min()\n",
    "\n",
    "        print(f\"Time ranges calculated for {len(max_time_nana)} trajectories\")\n",
    "\n",
    "        # Define a custom dataset\n",
    "        class TrajectoryDataset(Dataset):\n",
    "            def __init__(self, sequences, types, labels):\n",
    "                self.sequences = torch.tensor(sequences, dtype=torch.float32).to(device)\n",
    "                self.types = torch.tensor(types, dtype=torch.int64).to(device)\n",
    "                self.labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
    "                \n",
    "            def __len__(self):\n",
    "                return len(self.labels)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                return self.sequences[idx], self.types[idx], self.labels[idx]\n",
    "\n",
    "        # Define the model\n",
    "        class TrajectoryModel(nn.Module):\n",
    "            def __init__(self, num_trajectory_types, number_of_features, sequence_length):\n",
    "                super(TrajectoryModel, self).__init__()\n",
    "                self.lstm = nn.LSTM(number_of_features, 50, bidirectional=True, batch_first=True)\n",
    "                self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
    "                self.embedding = nn.Embedding(num_trajectory_types, 2)\n",
    "                self.flatten = nn.Flatten()\n",
    "                self.fc1 = nn.Linear(100 + 2, 100)  # 100 from LSTM + 2 from embedding\n",
    "                self.fc_mean = nn.Linear(100, 1)  # Mean prediction\n",
    "                self.fc_logvar = nn.Linear(100, 1)  # Log-variance\n",
    "                \n",
    "            def forward(self, x, type_input):\n",
    "                lstm_out, _ = self.lstm(x)\n",
    "                lstm_out = lstm_out.permute(0, 2, 1)  # (batch_size, num_features, seq_len)\n",
    "                avg_pool_out = self.avg_pool(lstm_out).squeeze(-1)  # (batch_size, 100)\n",
    "                embedded_type = self.embedding(type_input)\n",
    "                embedded_type = self.flatten(embedded_type)\n",
    "                combined = torch.cat((avg_pool_out, embedded_type), dim=1)\n",
    "                x = torch.relu(self.fc1(combined))\n",
    "                mu = self.fc_mean(x)  # Mean prediction\n",
    "                logvar = self.fc_logvar(x)  # Log-variance\n",
    "                return mu, logvar\n",
    "\n",
    "        # Scale the data - Column-wise scaling across all trajectories\n",
    "        def scale_trajectories_columnwise(trajectories):\n",
    "            \"\"\"\n",
    "            Scale each feature column separately across all trajectories\n",
    "            \"\"\"\n",
    "            if not trajectories:\n",
    "                return trajectories\n",
    "            \n",
    "            # Convert to numpy array for easier manipulation\n",
    "            all_trajectories = np.array(trajectories, dtype=object)\n",
    "            \n",
    "            # Get the number of features from the first trajectory\n",
    "            num_features = trajectories[0].shape[1] if len(trajectories[0]) > 0 else 0\n",
    "            \n",
    "            if num_features == 0:\n",
    "                return trajectories\n",
    "            \n",
    "            # Calculate global statistics for each feature column\n",
    "            all_values_per_feature = [[] for _ in range(num_features)]\n",
    "            \n",
    "            # Collect all values for each feature across all trajectories\n",
    "            for traj in trajectories:\n",
    "                if len(traj) > 0:\n",
    "                    for feature_idx in range(num_features):\n",
    "                        all_values_per_feature[feature_idx].extend(traj[:, feature_idx])\n",
    "            \n",
    "            # Calculate mean and std for each feature\n",
    "            feature_means = []\n",
    "            feature_stds = []\n",
    "            \n",
    "            for feature_idx in range(num_features):\n",
    "                values = np.array(all_values_per_feature[feature_idx])\n",
    "                mean_val = np.mean(values)\n",
    "                std_val = np.std(values)\n",
    "                \n",
    "                feature_means.append(mean_val)\n",
    "                feature_stds.append(std_val if std_val > 0 else 1.0)  # Avoid division by zero\n",
    "            \n",
    "            feature_means = np.array(feature_means)\n",
    "            feature_stds = np.array(feature_stds)\n",
    "            \n",
    "            # Scale each trajectory using global statistics\n",
    "            scaled_trajectories = []\n",
    "            for traj in trajectories:\n",
    "                if len(traj) == 0:\n",
    "                    scaled_trajectories.append(traj)\n",
    "                else:\n",
    "                    scaled_traj = (traj - feature_means) / feature_stds\n",
    "                    scaled_trajectories.append(scaled_traj)\n",
    "            \n",
    "            return scaled_trajectories, feature_means, feature_stds\n",
    "\n",
    "        # Prepare Nana's data for evaluation\n",
    "        print(\"Preprocessing Nana's data...\")\n",
    "\n",
    "        # Use the same features as the training data\n",
    "        features = ['rotation_x', 'rotation_y','rotation_z','normalized_time','velocity','angle','distance_to_end']\n",
    "\n",
    "\n",
    "        # Prepare trajectory data for Nana\n",
    "        X_nana = df_features_nana.drop(['type_trajectory'], axis=1)\n",
    "        grouped_nana = X_nana.groupby('id')\n",
    "        trajectories_nana = [group[features].values for _, group in grouped_nana]\n",
    "        ids_nana = list(grouped_nana.groups.keys())\n",
    "\n",
    "        # Get trajectory types for Nana\n",
    "        trajectory_type_nana = df_features_nana.groupby('id')['type_trajectory'].first().values\n",
    "        num_trajectory_types_nana = len(np.unique(trajectory_type_nana))\n",
    "        # Map Nana trajectory types to the same indices used in training\n",
    "\n",
    "        trajectory_type = df_features_nana.groupby('id')['type_trajectory'].first().values\n",
    "        unique_trajectory_types = np.unique(trajectory_type)\n",
    "        trajectory_type_to_index = {t: i for i, t in enumerate(unique_trajectory_types)}\n",
    "        trajectory_type_indices_nana = []\n",
    "        for t in trajectory_type_nana:\n",
    "            if t in trajectory_type_to_index:\n",
    "                trajectory_type_indices_nana.append(trajectory_type_to_index[t])\n",
    "            else:\n",
    "                print(f\"Warning: Trajectory type '{t}' not seen in training data. Using index 0.\")\n",
    "                trajectory_type_indices_nana.append(0)\n",
    "\n",
    "        trajectory_type_indices_nana = np.array(trajectory_type_indices_nana)\n",
    "        X_nana_scaled , thina_means, thina_stds = scale_trajectories_columnwise(trajectories_nana)\n",
    "\n",
    "\n",
    "        def pad_sequences(sequences, maxlen, dtype='float32', padding='post', value=0):\n",
    "            padded_sequences = np.full((len(sequences), maxlen, len(sequences[0][0])), value, dtype=dtype)\n",
    "            for i, seq in enumerate(sequences):\n",
    "                if len(seq) > maxlen:\n",
    "                    padded_sequences[i] = seq[:maxlen]\n",
    "                else:\n",
    "                    padded_sequences[i, :len(seq)] = seq\n",
    "            return padded_sequences\n",
    "        ### CHANGE TO SCALED!!!\n",
    "        max_sequence_length_nana = max([len(sequence) for sequence in trajectories_nana])\n",
    "        X_nana_padded = pad_sequences(trajectories_nana, max_sequence_length_nana, padding='post')\n",
    "\n",
    "        sequence_length_nana = X_nana_padded.shape[1]\n",
    "        number_of_features_nana = X_nana_padded.shape[2]\n",
    "\n",
    "        # Check MPS availability\n",
    "        device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "        print(\"Using device:\", device)\n",
    "        # Pad sequences using the same max_sequence_length from training\n",
    "        print(f\"Padded Nana trajectories shape: {X_nana_padded.shape}\")\n",
    "        print(f\"Trajectory types distribution in Nana:\")\n",
    "        unique_nana, counts_nana = np.unique(trajectory_type_nana, return_counts=True)\n",
    "        for t, c in zip(unique_nana, counts_nana):\n",
    "            print(f\"  {t}: {c} trajectories\")\n",
    "\n",
    "        # Evaluate on Nana's data\n",
    "        print(\"Evaluating trained model on Nana's data...\")\n",
    "\n",
    "        # Use the last trained model instance directly\n",
    "        model_eval = model\n",
    "        model_eval.eval()\n",
    "\n",
    "        # Prepare labels for Nana\n",
    "        y_nana = np.array([start_move_nana[_id] for _id in ids_nana])\n",
    "\n",
    "        # Create dataset and dataloader for Nana\n",
    "        nana_dataset = TrajectoryDataset(X_nana_padded, trajectory_type_indices_nana, y_nana, device)\n",
    "        nana_loader = DataLoader(nana_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "        # Evaluate the model\n",
    "        predictions_nana = {}\n",
    "        nana_mae_list = []\n",
    "\n",
    "        print(\"Making predictions on Nana's data...\")\n",
    "        with torch.no_grad():\n",
    "            index_offset = 0\n",
    "            for sequences, types, labels in nana_loader:\n",
    "                sequences, types, labels = sequences.to(device), types.to(device), labels.to(device)\n",
    "                mu, logvar = model_eval(sequences, types)\n",
    "                batch_size = sequences.size(0)\n",
    "                batch_ids = ids_nana[index_offset:index_offset + batch_size]\n",
    "                index_offset += batch_size\n",
    "                for _id, pred_mu, pred_logvar in zip(batch_ids, mu.squeeze().tolist(), logvar.squeeze().tolist()):\n",
    "                    predictions_nana[_id] = {\"mu\": pred_mu, \"logvar\": pred_logvar}\n",
    "                    true_value_ms = start_move_nana[_id] * (max_time_nana[_id] - min_time_nana[_id]) + min_time_nana[_id]\n",
    "                    pred_value_ms = pred_mu * (max_time_nana[_id] - min_time_nana[_id]) + min_time_nana[_id]\n",
    "                    mae = abs(pred_value_ms - true_value_ms)\n",
    "                    nana_mae_list.append(mae)\n",
    "        print(f\"Evaluation completed on {len(predictions_nana)} Nana trajectories\")\n",
    "\n",
    "        # Analyze Nana evaluation results\n",
    "        print(\"Analyzing results on Nana's data...\")\n",
    "\n",
    "        # Convert predictions to DataFrame\n",
    "        df_results_nana = pd.DataFrame.from_dict(predictions_nana, orient='index')\n",
    "        df_results_nana['variance'] = np.exp(df_results_nana['logvar'])\n",
    "        df_results_nana['true_y'] = [start_move_nana[_id] for _id in df_results_nana.index]\n",
    "\n",
    "        # Add trajectory types\n",
    "        trajectory_types_nana = df_features_nana.groupby(\"id\")[\"type_trajectory\"].first().to_dict()\n",
    "        df_results_nana['type_trajectory'] = df_results_nana.index.map(trajectory_types_nana)\n",
    "        df_results_nana['absolute_error'] = abs(df_results_nana['mu'] - df_results_nana['true_y'])\n",
    "        df_results_nana['residuals'] = df_results_nana['true_y'] - df_results_nana['mu']\n",
    "\n",
    "        # Print evaluation metrics\n",
    "        print(\"\\n=== EVALUATION ON NANA'S DATA ===\")\n",
    "        print(f\"Number of trajectories: {len(df_results_nana)}\")\n",
    "        print(f\"MAE: {df_results_nana['absolute_error'].mean():.4f}\")\n",
    "        print(f\"RMSE: {np.sqrt((df_results_nana['residuals']**2).mean()):.4f}\")\n",
    "\n",
    "        print(\"\\nMAE by trajectory type:\")\n",
    "        for traj_type in df_results_nana['type_trajectory'].unique():\n",
    "            subset = df_results_nana[df_results_nana['type_trajectory'] == traj_type]\n",
    "            print(f\"  {traj_type}: {subset['absolute_error'].mean():.4f} (n={len(subset)})\")\n",
    "\n",
    "        # Filter the DataFrame to include only rows where both true_y and mu are between 0 and 1\n",
    "        filtered_df = df_results_nana[\n",
    "            (df_results_nana['true_y'] >= 0) & (df_results_nana['true_y'] <= 1) &\n",
    "            (df_results_nana['mu'] >= 0) & (df_results_nana['mu'] <= 1)\n",
    "        ]\n",
    "\n",
    "        # Plotting\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.scatter(filtered_df['true_y'], filtered_df['mu'], alpha=0.6)\n",
    "        plt.xlabel(\"True Values\")\n",
    "        plt.ylabel(\"Predicted Values\")\n",
    "        plt.title(\"True vs Predicted Values - Movement Onset point (Nana's Data) Trained on Thina\")\n",
    "        plt.plot([0, 1], [0, 1], color='red', label='Identity Line')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "print(\"\\nMAE results for 10 runs:\")\n",
    "print(mae_results)\n",
    "print(f\"Mean MAE over 10 runs: {np.mean(mae_results):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
